# -*- coding: utf-8 -*-
"""EnviroSort.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIJmAkW_Eh3Q81eFOLuAFMQU9OF4BVty
"""

import os
import shutil
os.chdir('/root/')
os.rename('kaggle.json', '/root/.kaggle/kaggle.json')
!pip install kaggle
os.chdir('/content/')
os.listdir('/content/')
!kaggle datasets download techsash/waste-classification-data
!unzip waste-classification-data
os.remove('waste-classification-data.zip')
shutil.rmtree('/content/dataset')

import os
import pandas as pd
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

# Enable eager execution
tf.config.run_functions_eagerly(True)

# Ensure GPU memory growth is enabled
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# Paths to the train and test directories
train_data_dir = "/content/DATASET/TRAIN/"
test_data_dir = "/content/DATASET/TEST/"

# Parameters for data augmentation and preprocessing
batch_size = 64
target_size = (128, 128)

# Data augmentation for the training set
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Only rescale the test set (no augmentation)
test_datagen = ImageDataGenerator(rescale=1./255)

# Load train and test data using ImageDataGenerator
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_data_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='binary'
)

# Load the VGG16 model without the top (fully connected) layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(*target_size, 3))

# Fine-tune the model by allowing some layers to be trainable
for layer in base_model.layers[:-4]:
    layer.trainable = False

# Create a new model on top of the base model
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model with run_eagerly=True
model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)

# View the model summary
model.summary()

# Train the model with a subset of data
data_subset_percentage = 1  # Modify this to specify the percentage of data to use (e.g., 0.1 for 10% of data)
num_samples_subset = int(len(train_generator) * data_subset_percentage)
train_generator_subset = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=target_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='training',
    seed=42,
    shuffle=True
)

# Train the model using more data
epochs = 30  # Increase the number of epochs for better convergence
steps_per_epoch_subset = num_samples_subset // batch_size
validation_steps = len(test_generator)
model.fit(
    train_generator_subset,
    steps_per_epoch=steps_per_epoch_subset,
    epochs=epochs,
    validation_data=test_generator,
    validation_steps=validation_steps
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator, steps=validation_steps)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)